---
layout: ../../layouts/post.astro
title: Multilingual LLM
description: Wall in scalling LLM?
dateFormatted: Jan , 2025
---


"Limitations of your words are limitations of your world"
 ~ philosopher Ludwig Wittgenstein

At NeurIPS, a world-famous AI conference, Ilya, an AI scientist, shared a new insight.

We are running out of training data. The Internet is a limited resource.
Hence, AI is not able to morph into AGI.

But, we have mostly utilized English data so far.

90% of training data for GPT-3 and LLaMA 2 was in the English language.

We are still to uncover the treasure of non-English languages.

Many books, articles, and publications in non-English languages are not even digitized.

Utilizing these languages can change the way data distribution looks under the hood.

LLMs are few-shot or, in better words, unsupervised competent learners.

It can help strengthen the tasks in which it currently appears weak.

The challenge is definitely if it's worth the effort. But we will never know until we try.

After all, Data Science is research, and research is trying something again with a fresh perspective.



## LinkedIn

https://www.linkedin.com/posts/pritampandit_languagemodels-artificialintelligence-modelinterpretation-activity-7205925934363766786-Qaog?utm_source=share&utm_medium=member_desktop
