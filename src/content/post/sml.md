---
layout: ../../layouts/post.astro
title: Small Language Models
description: Efficiency of SLM
dateFormatted: Dec , 2024
---

1B parameters is all you need.

Andrej karpathy has dropped another truth bomb about LLMs.
Models equal or smaller than 1B are enough for Enterprise use cases. Because they do not need all information present in Large Language models.
Like History, Travel guide etc.

Hence, Model distillation can get the same cognitive abilities in smaller models.
Smaller models also boosts both inference speed and cost savings.

Researches' from Oxford University found an interesting result in their study.
7B parameters have lower hallucination than GPT-3-175B. Means they are more accurate than large models.


## LinkedIn

https://www.linkedin.com/posts/pritampandit_slm-genrativeai-smallcon-activity-7270917388005486593-iLir?utm_source=share&utm_medium=member_desktop

